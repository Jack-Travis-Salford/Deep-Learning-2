{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read and has shape (20491, 2)\n",
      "                                                  Review  Rating\n",
      "0      nice hotel expensive parking got good deal sta...       4\n",
      "1      ok nothing special charge diamond member hilto...       2\n",
      "2      nice rooms not 4* experience hotel monaco seat...       3\n",
      "3      unique, great stay, wonderful time hotel monac...       5\n",
      "4      great stay great stay, went seahawk game aweso...       5\n",
      "...                                                  ...     ...\n",
      "20486  best kept secret 3rd time staying charm, not 5...       5\n",
      "20487  great location price view hotel great quick pl...       4\n",
      "20488  ok just looks nice modern outside, desk staff ...       2\n",
      "20489  hotel theft ruined vacation hotel opened sept ...       1\n",
      "20490  people talking, ca n't believe excellent ratin...       2\n",
      "\n",
      "[20491 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "dataFile = pd.read_csv(\"Data File/tripadvisor_hotel_reviews.csv\")\n",
    "\n",
    "#dataFile['Rating'] = dataFile.Rating.astype('category')\n",
    "#print(dataFile.dtypes)\n",
    "\n",
    "print(\"Data read and has shape\", dataFile.shape)\n",
    "print(dataFile)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T09:54:46.685044Z",
     "end_time": "2023-05-05T09:54:47.642464Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "no_reviews = 20491    # no of reviews that will be read from file.\n",
    "max_review_length = 500 # no of words per review.  reviews will be  truncated or padded to be of this length.\n",
    "max_words = 52212        # this is the size of the index (i.e. most common top words that will be used as features)\n",
    "# note code assumes there are enough words in reviews.\n",
    "embedding_dim = 100     # length of embedding based on Glove\n",
    "validation_prop = 0.2   # prop of data for validation set\n",
    "no_epochs =   20         # No of training cycles for the networks\n",
    "batch_size = 64        # batch size for training\n",
    "\n",
    "training_samples = 12000\n",
    "validation_samples = 7000"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T09:54:47.646541Z",
     "end_time": "2023-05-05T09:54:47.685668Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "# Use the tokenizer to code the reviews\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "reviews = dataFile[\"Review\"].values\n",
    "ratings = dataFile[\"Rating\"].values\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "sequences = tokenizer.texts_to_sequences(reviews)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "dataFile = pad_sequences(sequences, maxlen=max_review_length)\n",
    "#print(f'Found {len(word_index)} unique tokens')\n",
    "\n",
    "labels = to_categorical(np.asarray(ratings-1))\n",
    "x_test = dataFile[19000:]\n",
    "y_test = labels[19000:]\n",
    "testExamples = len(labels)-19000\n",
    "x_train, x_val, y_train, y_val= train_test_split(dataFile[:19000], labels[:19000], test_size=0.2, random_state=42)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T09:54:47.664049Z",
     "end_time": "2023-05-05T09:54:58.970545Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of words in glove embeddings = 400000\n"
     ]
    }
   ],
   "source": [
    "glove_dir = \".\\\\Glove\\\\glove.6B\"\n",
    "embeddings_index = {}\n",
    "\n",
    "f = open(os.path.join(glove_dir,'glove.6B.100d.txt'),encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype = 'float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('no of words in glove embeddings =', len(embeddings_index))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T09:54:58.975545Z",
     "end_time": "2023-05-05T09:55:14.836043Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of embeddings matrix is: (52212, 100)\n",
      "1:hotel\t--> [ 0.43044001 -0.71715999  0.13989     0.59311002 -0.16727     0.56128001]\n",
      "2:room\t--> [-0.024843    0.47766     0.32437    -0.054239   -0.47622001  1.10430002]\n",
      "3:not\t--> [-0.19103999  0.17601     0.36919999 -0.50322998 -0.47560999  0.15798   ]\n",
      "4:great\t--> [-0.013786    0.38216001  0.53236002  0.15261    -0.29694    -0.20558   ]\n",
      "5:n't\t--> [ 0.15730999  0.3953      0.63586003 -1.09749997 -0.95767999 -0.013841  ]\n",
      "6:good\t--> [-0.030769    0.11993     0.53908998 -0.43696001 -0.73936999 -0.15345   ]\n",
      "7:staff\t--> [-0.61250001 -0.29506999 -0.28917    -0.36431    -0.39695001  0.097624  ]\n",
      "8:stay\t--> [-0.41615999 -0.26538     0.21720999 -0.26014999 -0.18043999  0.38745001]\n",
      "9:did\t--> [ 0.30449    -0.19628     0.20225    -0.61686999 -0.68484002 -0.11887   ]\n",
      "10:just\t--> [ 0.075026    0.39324999  0.90314001 -0.30451    -0.32767999  0.59630001]\n"
     ]
    }
   ],
   "source": [
    "#look for word embeddings\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "\n",
    "for word,i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "print(\"shape of embeddings matrix is:\",  embedding_matrix.shape)\n",
    "\n",
    "# print some entries\n",
    "\n",
    "for word,i in word_index.items():\n",
    "    if i > 10: break\n",
    "    print(f'{i}:{word}\\t--> { embedding_matrix[i, 0:6]}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T09:55:14.836043Z",
     "end_time": "2023-05-05T09:55:14.914591Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 100)          5221200   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                42240     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,263,765\n",
      "Trainable params: 42,565\n",
      "Non-trainable params: 5,221,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, Dense, LSTM\n",
    "from keras import layers\n",
    "\n",
    "network_G = Sequential()\n",
    "network_G.add(Embedding(len(word_index)+1, embedding_dim, weights=[embedding_matrix], input_length=max_review_length, trainable=False))\n",
    "network_G.add(LSTM(64))\n",
    "network_G.add(Dense(5, activation='softmax'))\n",
    "network_G.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T09:55:14.914591Z",
     "end_time": "2023-05-05T09:55:15.505618Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "238/238 [==============================] - 76s 311ms/step - loss: 1.2898 - acc: 0.4541 - val_loss: 1.2449 - val_acc: 0.4663\n",
      "Epoch 2/20\n",
      "238/238 [==============================] - 71s 299ms/step - loss: 1.0833 - acc: 0.5266 - val_loss: 1.1472 - val_acc: 0.4958\n",
      "Epoch 3/20\n",
      "238/238 [==============================] - 71s 298ms/step - loss: 0.9672 - acc: 0.5782 - val_loss: 0.9468 - val_acc: 0.5855\n",
      "Epoch 4/20\n",
      "238/238 [==============================] - 71s 299ms/step - loss: 0.9008 - acc: 0.6022 - val_loss: 0.8915 - val_acc: 0.6042\n",
      "Epoch 5/20\n",
      "238/238 [==============================] - 72s 303ms/step - loss: 0.8582 - acc: 0.6211 - val_loss: 0.8808 - val_acc: 0.6082\n",
      "Epoch 6/20\n",
      "238/238 [==============================] - 72s 302ms/step - loss: 0.8264 - acc: 0.6361 - val_loss: 0.8645 - val_acc: 0.6068\n",
      "Epoch 7/20\n",
      "238/238 [==============================] - 83s 350ms/step - loss: 0.8063 - acc: 0.6459 - val_loss: 0.8643 - val_acc: 0.6108\n",
      "Epoch 8/20\n",
      "238/238 [==============================] - 97s 406ms/step - loss: 0.7762 - acc: 0.6574 - val_loss: 0.8357 - val_acc: 0.6268\n",
      "Epoch 9/20\n",
      "238/238 [==============================] - 87s 368ms/step - loss: 0.7622 - acc: 0.6645 - val_loss: 0.8260 - val_acc: 0.6413\n",
      "Epoch 10/20\n",
      "238/238 [==============================] - 71s 297ms/step - loss: 0.7417 - acc: 0.6751 - val_loss: 0.8350 - val_acc: 0.6318\n",
      "Epoch 11/20\n",
      "238/238 [==============================] - 70s 296ms/step - loss: 0.7250 - acc: 0.6789 - val_loss: 0.8309 - val_acc: 0.6408\n",
      "Epoch 12/20\n",
      "238/238 [==============================] - 76s 319ms/step - loss: 0.7045 - acc: 0.6963 - val_loss: 0.8325 - val_acc: 0.6292\n",
      "Epoch 13/20\n",
      "238/238 [==============================] - 61s 255ms/step - loss: 0.6895 - acc: 0.7015 - val_loss: 0.8405 - val_acc: 0.6271\n",
      "Epoch 14/20\n",
      "238/238 [==============================] - 60s 252ms/step - loss: 0.6643 - acc: 0.7137 - val_loss: 0.8514 - val_acc: 0.6392\n",
      "Epoch 15/20\n",
      "238/238 [==============================] - 63s 264ms/step - loss: 0.6394 - acc: 0.7284 - val_loss: 0.8515 - val_acc: 0.6311\n",
      "Epoch 16/20\n",
      "238/238 [==============================] - 59s 248ms/step - loss: 0.6241 - acc: 0.7361 - val_loss: 0.8646 - val_acc: 0.6279\n",
      "Epoch 17/20\n",
      "238/238 [==============================] - 59s 248ms/step - loss: 0.5964 - acc: 0.7464 - val_loss: 0.8770 - val_acc: 0.6266\n",
      "Epoch 18/20\n",
      "238/238 [==============================] - 59s 248ms/step - loss: 0.5789 - acc: 0.7572 - val_loss: 0.9098 - val_acc: 0.6313\n",
      "Epoch 19/20\n",
      "238/238 [==============================] - 59s 247ms/step - loss: 0.5524 - acc: 0.7708 - val_loss: 0.9070 - val_acc: 0.6163\n",
      "Epoch 20/20\n",
      "238/238 [==============================] - 59s 249ms/step - loss: 0.5326 - acc: 0.7776 - val_loss: 0.9240 - val_acc: 0.6247\n"
     ]
    }
   ],
   "source": [
    "network_G.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'] )\n",
    "\n",
    "hist_g = network_G.fit(x_train,y_train, epochs=no_epochs, batch_size=batch_size, validation_data= (x_val,y_val))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T09:55:15.489990Z",
     "end_time": "2023-05-05T10:18:31.913921Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1491/1491 [==============================] - 23s 16ms/step - loss: 0.8621 - acc: 0.6385\n",
      "Accuracy on test set: 0.638\n"
     ]
    }
   ],
   "source": [
    "res = network_G.evaluate(x_test, y_test, steps=testExamples, verbose=1)\n",
    "print('Accuracy on test set: %.3f' % res[1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T10:18:31.915922Z",
     "end_time": "2023-05-05T10:18:55.168109Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T10:18:55.170106Z",
     "end_time": "2023-05-05T10:18:55.186135Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
