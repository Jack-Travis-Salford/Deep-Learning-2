{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read and has shape (20491, 2)\n",
      "                                                  Review  Rating\n",
      "0      nice hotel expensive parking got good deal sta...       4\n",
      "1      ok nothing special charge diamond member hilto...       2\n",
      "2      nice rooms not 4* experience hotel monaco seat...       3\n",
      "3      unique, great stay, wonderful time hotel monac...       5\n",
      "4      great stay great stay, went seahawk game aweso...       5\n",
      "...                                                  ...     ...\n",
      "20486  best kept secret 3rd time staying charm, not 5...       5\n",
      "20487  great location price view hotel great quick pl...       4\n",
      "20488  ok just looks nice modern outside, desk staff ...       2\n",
      "20489  hotel theft ruined vacation hotel opened sept ...       1\n",
      "20490  people talking, ca n't believe excellent ratin...       2\n",
      "\n",
      "[20491 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "dataFile = pd.read_csv(\"Data File/tripadvisor_hotel_reviews.csv\")\n",
    "\n",
    "#dataFile['Rating'] = dataFile.Rating.astype('category')\n",
    "#print(dataFile.dtypes)\n",
    "\n",
    "print(\"Data read and has shape\", dataFile.shape)\n",
    "print(dataFile)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T08:27:15.963808Z",
     "end_time": "2023-05-05T08:27:16.523279Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "no_reviews = 20491    # no of reviews that will be read from file.\n",
    "max_review_length = 500 # no of words per review.  reviews will be  truncated or padded to be of this length.\n",
    "max_words = 52212        # this is the size of the index (i.e. most common top words that will be used as features)\n",
    "# note code assumes there are enough words in reviews.\n",
    "embedding_dim = 100     # length of embedding based on Glove\n",
    "validation_prop = 0.2   # prop of data for validation set\n",
    "no_epochs =   10         # No of training cycles for the networks\n",
    "batch_size = 64        # batch size for training\n",
    "\n",
    "training_samples = 12000\n",
    "validation_samples = 7000"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T08:27:16.527280Z",
     "end_time": "2023-05-05T08:27:16.539673Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "# Use the tokenizer to code the reviews\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "reviews = dataFile[\"Review\"].values\n",
    "ratings = dataFile[\"Rating\"].values\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "sequences = tokenizer.texts_to_sequences(reviews)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "dataFile = pad_sequences(sequences, maxlen=max_review_length)\n",
    "#print(f'Found {len(word_index)} unique tokens')\n",
    "\n",
    "labels = to_categorical(np.asarray(ratings-1))\n",
    "x_test = dataFile[19000:]\n",
    "y_test = labels[19000:]\n",
    "testExamples = len(labels)-19000\n",
    "x_train, x_val, y_train, y_val= train_test_split(dataFile[:19000], labels[:19000], test_size=0.2, random_state=42)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T08:27:16.542703Z",
     "end_time": "2023-05-05T08:27:22.816302Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of words in glove embeddings = 400000\n"
     ]
    }
   ],
   "source": [
    "glove_dir = \".\\\\Glove\\\\glove.6B\"\n",
    "embeddings_index = {}\n",
    "\n",
    "f = open(os.path.join(glove_dir,'glove.6B.100d.txt'),encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype = 'float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('no of words in glove embeddings =', len(embeddings_index))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T08:27:22.819405Z",
     "end_time": "2023-05-05T08:27:32.410854Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of embeddings matrix is: (52212, 100)\n",
      "1:hotel\t--> [ 0.43044001 -0.71715999  0.13989     0.59311002 -0.16727     0.56128001]\n",
      "2:room\t--> [-0.024843    0.47766     0.32437    -0.054239   -0.47622001  1.10430002]\n",
      "3:not\t--> [-0.19103999  0.17601     0.36919999 -0.50322998 -0.47560999  0.15798   ]\n",
      "4:great\t--> [-0.013786    0.38216001  0.53236002  0.15261    -0.29694    -0.20558   ]\n",
      "5:n't\t--> [ 0.15730999  0.3953      0.63586003 -1.09749997 -0.95767999 -0.013841  ]\n",
      "6:good\t--> [-0.030769    0.11993     0.53908998 -0.43696001 -0.73936999 -0.15345   ]\n",
      "7:staff\t--> [-0.61250001 -0.29506999 -0.28917    -0.36431    -0.39695001  0.097624  ]\n",
      "8:stay\t--> [-0.41615999 -0.26538     0.21720999 -0.26014999 -0.18043999  0.38745001]\n",
      "9:did\t--> [ 0.30449    -0.19628     0.20225    -0.61686999 -0.68484002 -0.11887   ]\n",
      "10:just\t--> [ 0.075026    0.39324999  0.90314001 -0.30451    -0.32767999  0.59630001]\n"
     ]
    }
   ],
   "source": [
    "#look for word embeddings\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "\n",
    "for word,i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "print(\"shape of embeddings matrix is:\",  embedding_matrix.shape)\n",
    "\n",
    "# print some entries\n",
    "\n",
    "for word,i in word_index.items():\n",
    "    if i > 10: break\n",
    "    print(f'{i}:{word}\\t--> { embedding_matrix[i, 0:6]}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T08:27:32.414859Z",
     "end_time": "2023-05-05T08:27:32.490214Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 100)          5221200   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                42240     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,263,765\n",
      "Trainable params: 42,565\n",
      "Non-trainable params: 5,221,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, Dense, LSTM\n",
    "from keras import layers\n",
    "\n",
    "network_G = Sequential()\n",
    "network_G.add(Embedding(len(word_index)+1, embedding_dim, weights=[embedding_matrix], input_length=max_review_length, trainable=False))\n",
    "network_G.add(LSTM(64))\n",
    "network_G.add(Dense(5, activation='softmax'))\n",
    "network_G.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T08:27:32.490214Z",
     "end_time": "2023-05-05T08:27:32.881394Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "238/238 [==============================] - 64s 263ms/step - loss: 1.2169 - acc: 0.4773 - val_loss: 1.4338 - val_acc: 0.4197\n",
      "Epoch 2/10\n",
      "238/238 [==============================] - 62s 261ms/step - loss: 1.2509 - acc: 0.4630 - val_loss: 1.1494 - val_acc: 0.4947\n",
      "Epoch 3/10\n",
      "238/238 [==============================] - 60s 253ms/step - loss: 1.0574 - acc: 0.5293 - val_loss: 1.0131 - val_acc: 0.5458\n",
      "Epoch 4/10\n",
      "238/238 [==============================] - 62s 262ms/step - loss: 0.9796 - acc: 0.5627 - val_loss: 0.9606 - val_acc: 0.5705\n",
      "Epoch 5/10\n",
      "238/238 [==============================] - 62s 261ms/step - loss: 0.9203 - acc: 0.5914 - val_loss: 0.9312 - val_acc: 0.5784\n",
      "Epoch 6/10\n",
      "238/238 [==============================] - 60s 252ms/step - loss: 0.8920 - acc: 0.6009 - val_loss: 0.9228 - val_acc: 0.5887\n",
      "Epoch 7/10\n",
      "238/238 [==============================] - 62s 263ms/step - loss: 0.8578 - acc: 0.6167 - val_loss: 0.8946 - val_acc: 0.6013\n",
      "Epoch 8/10\n",
      "238/238 [==============================] - 62s 260ms/step - loss: 0.8356 - acc: 0.6325 - val_loss: 0.8766 - val_acc: 0.6071\n",
      "Epoch 9/10\n",
      "238/238 [==============================] - 62s 259ms/step - loss: 0.8188 - acc: 0.6358 - val_loss: 0.8656 - val_acc: 0.6179\n",
      "Epoch 10/10\n",
      "238/238 [==============================] - 62s 259ms/step - loss: 0.8008 - acc: 0.6482 - val_loss: 0.8556 - val_acc: 0.6271\n"
     ]
    }
   ],
   "source": [
    "network_G.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'] )\n",
    "\n",
    "hist_g = network_G.fit(x_train,y_train, epochs=no_epochs, batch_size=batch_size, validation_data= (x_val,y_val))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T08:27:32.884399Z",
     "end_time": "2023-05-05T08:37:50.813866Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1491/1491 [==============================] - 23s 16ms/step - loss: 0.8282 - acc: 0.6412\n",
      "Accuracy on test set: 0.641\n"
     ]
    }
   ],
   "source": [
    "res = network_G.evaluate(x_test, y_test, steps=testExamples, verbose=1)\n",
    "print('Accuracy on test set: %.3f' % res[1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T08:37:50.813866Z",
     "end_time": "2023-05-05T08:38:14.086926Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T08:38:14.086926Z",
     "end_time": "2023-05-05T08:38:14.102551Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
