{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read and has shape (20491, 2)\n",
      "                                                  Review  Rating\n",
      "0      nice hotel expensive parking got good deal sta...       4\n",
      "1      ok nothing special charge diamond member hilto...       2\n",
      "2      nice rooms not 4* experience hotel monaco seat...       3\n",
      "3      unique, great stay, wonderful time hotel monac...       5\n",
      "4      great stay great stay, went seahawk game aweso...       5\n",
      "...                                                  ...     ...\n",
      "20486  best kept secret 3rd time staying charm, not 5...       5\n",
      "20487  great location price view hotel great quick pl...       4\n",
      "20488  ok just looks nice modern outside, desk staff ...       2\n",
      "20489  hotel theft ruined vacation hotel opened sept ...       1\n",
      "20490  people talking, ca n't believe excellent ratin...       2\n",
      "\n",
      "[20491 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "dataFile = pd.read_csv(\"Data File/tripadvisor_hotel_reviews.csv\")\n",
    "\n",
    "#dataFile['Rating'] = dataFile.Rating.astype('category')\n",
    "#print(dataFile.dtypes)\n",
    "\n",
    "print(\"Data read and has shape\", dataFile.shape)\n",
    "print(dataFile)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T10:03:08.461125Z",
     "end_time": "2023-05-05T10:03:10.337418Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "no_reviews = 20491    # no of reviews that will be read from file.\n",
    "max_review_length = 500 # no of words per review.  reviews will be  truncated or padded to be of this length.\n",
    "max_words = 52212        # this is the size of the index (i.e. most common top words that will be used as features)\n",
    "# note code assumes there are enough words in reviews.\n",
    "embedding_dim = 100     # length of embedding based on Glove\n",
    "validation_prop = 0.2   # prop of data for validation set\n",
    "no_epochs =   20         # No of training cycles for the networks\n",
    "batch_size = 64        # batch size for training\n",
    "\n",
    "training_samples = 12000\n",
    "validation_samples = 7000"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T10:03:10.339419Z",
     "end_time": "2023-05-05T10:03:10.358719Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "# Use the tokenizer to code the reviews\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "reviews = dataFile[\"Review\"].values\n",
    "ratings = dataFile[\"Rating\"].values\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "sequences = tokenizer.texts_to_sequences(reviews)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "dataFile = pad_sequences(sequences, maxlen=max_review_length)\n",
    "#print(f'Found {len(word_index)} unique tokens')\n",
    "\n",
    "labels = to_categorical(np.asarray(ratings-1))\n",
    "x_test = dataFile[19000:]\n",
    "y_test = labels[19000:]\n",
    "testExamples = len(labels)-19000\n",
    "x_train, x_val, y_train, y_val= train_test_split(dataFile[:19000], labels[:19000], test_size=0.2, random_state=42)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T10:03:10.356717Z",
     "end_time": "2023-05-05T10:03:36.700251Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of words in glove embeddings = 400000\n"
     ]
    }
   ],
   "source": [
    "glove_dir = \".\\\\Glove\\\\glove.6B\"\n",
    "embeddings_index = {}\n",
    "\n",
    "f = open(os.path.join(glove_dir,'glove.6B.100d.txt'),encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype = 'float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('no of words in glove embeddings =', len(embeddings_index))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T10:03:36.699252Z",
     "end_time": "2023-05-05T10:04:11.019946Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of embeddings matrix is: (52212, 100)\n",
      "1:hotel\t--> [ 0.43044001 -0.71715999  0.13989     0.59311002 -0.16727     0.56128001]\n",
      "2:room\t--> [-0.024843    0.47766     0.32437    -0.054239   -0.47622001  1.10430002]\n",
      "3:not\t--> [-0.19103999  0.17601     0.36919999 -0.50322998 -0.47560999  0.15798   ]\n",
      "4:great\t--> [-0.013786    0.38216001  0.53236002  0.15261    -0.29694    -0.20558   ]\n",
      "5:n't\t--> [ 0.15730999  0.3953      0.63586003 -1.09749997 -0.95767999 -0.013841  ]\n",
      "6:good\t--> [-0.030769    0.11993     0.53908998 -0.43696001 -0.73936999 -0.15345   ]\n",
      "7:staff\t--> [-0.61250001 -0.29506999 -0.28917    -0.36431    -0.39695001  0.097624  ]\n",
      "8:stay\t--> [-0.41615999 -0.26538     0.21720999 -0.26014999 -0.18043999  0.38745001]\n",
      "9:did\t--> [ 0.30449    -0.19628     0.20225    -0.61686999 -0.68484002 -0.11887   ]\n",
      "10:just\t--> [ 0.075026    0.39324999  0.90314001 -0.30451    -0.32767999  0.59630001]\n"
     ]
    }
   ],
   "source": [
    "#look for word embeddings\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "\n",
    "for word,i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "print(\"shape of embeddings matrix is:\",  embedding_matrix.shape)\n",
    "\n",
    "# print some entries\n",
    "\n",
    "for word,i in word_index.items():\n",
    "    if i > 10: break\n",
    "    print(f'{i}:{word}\\t--> { embedding_matrix[i, 0:6]}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T10:04:11.014770Z",
     "end_time": "2023-05-05T10:04:11.237851Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 100)          5221200   \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 64)                10560     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,232,085\n",
      "Trainable params: 10,885\n",
      "Non-trainable params: 5,221,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, Dense\n",
    "from keras import layers\n",
    "\n",
    "network_G = Sequential()\n",
    "network_G.add(Embedding(len(word_index)+1, embedding_dim, weights=[embedding_matrix], input_length=max_review_length, trainable=False))\n",
    "network_G.add(SimpleRNN(64))\n",
    "network_G.add(Dense(5, activation='softmax'))\n",
    "network_G.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T10:04:11.213490Z",
     "end_time": "2023-05-05T10:04:12.001460Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "238/238 [==============================] - 28s 104ms/step - loss: 1.3723 - acc: 0.4283 - val_loss: 1.3170 - val_acc: 0.4418\n",
      "Epoch 2/20\n",
      "238/238 [==============================] - 24s 102ms/step - loss: 1.2913 - acc: 0.4510 - val_loss: 1.2704 - val_acc: 0.4613\n",
      "Epoch 3/20\n",
      "238/238 [==============================] - 22s 92ms/step - loss: 1.3193 - acc: 0.4459 - val_loss: 1.3070 - val_acc: 0.4466\n",
      "Epoch 4/20\n",
      "238/238 [==============================] - 22s 93ms/step - loss: 1.2557 - acc: 0.4705 - val_loss: 1.2836 - val_acc: 0.4511\n",
      "Epoch 5/20\n",
      "238/238 [==============================] - 20s 84ms/step - loss: 1.3509 - acc: 0.4463 - val_loss: 1.3523 - val_acc: 0.4387\n",
      "Epoch 6/20\n",
      "238/238 [==============================] - 21s 88ms/step - loss: 1.3354 - acc: 0.4487 - val_loss: 1.3416 - val_acc: 0.4432\n",
      "Epoch 7/20\n",
      "238/238 [==============================] - 20s 84ms/step - loss: 1.3238 - acc: 0.4517 - val_loss: 1.3466 - val_acc: 0.4413\n",
      "Epoch 8/20\n",
      "238/238 [==============================] - 18s 75ms/step - loss: 1.3124 - acc: 0.4553 - val_loss: 1.3284 - val_acc: 0.4376\n",
      "Epoch 9/20\n",
      "238/238 [==============================] - 17s 71ms/step - loss: 1.3000 - acc: 0.4582 - val_loss: 1.3143 - val_acc: 0.4471\n",
      "Epoch 10/20\n",
      "238/238 [==============================] - 17s 70ms/step - loss: 1.2846 - acc: 0.4635 - val_loss: 1.3002 - val_acc: 0.4432\n",
      "Epoch 11/20\n",
      "238/238 [==============================] - 17s 70ms/step - loss: 1.2523 - acc: 0.4701 - val_loss: 1.2527 - val_acc: 0.4687\n",
      "Epoch 12/20\n",
      "238/238 [==============================] - 17s 71ms/step - loss: 1.2468 - acc: 0.4664 - val_loss: 1.2464 - val_acc: 0.4576\n",
      "Epoch 13/20\n",
      "238/238 [==============================] - 17s 71ms/step - loss: 1.2116 - acc: 0.4783 - val_loss: 1.2233 - val_acc: 0.4511\n",
      "Epoch 14/20\n",
      "238/238 [==============================] - 17s 71ms/step - loss: 1.1989 - acc: 0.4868 - val_loss: 1.2289 - val_acc: 0.4511\n",
      "Epoch 15/20\n",
      "238/238 [==============================] - 17s 70ms/step - loss: 1.3007 - acc: 0.4482 - val_loss: 1.3390 - val_acc: 0.4329\n",
      "Epoch 16/20\n",
      "238/238 [==============================] - 17s 70ms/step - loss: 1.2988 - acc: 0.4443 - val_loss: 1.3018 - val_acc: 0.4408\n",
      "Epoch 17/20\n",
      "238/238 [==============================] - 17s 71ms/step - loss: 1.2471 - acc: 0.4616 - val_loss: 1.2341 - val_acc: 0.4545\n",
      "Epoch 18/20\n",
      "238/238 [==============================] - 17s 70ms/step - loss: 1.1998 - acc: 0.4797 - val_loss: 1.2023 - val_acc: 0.4695\n",
      "Epoch 19/20\n",
      "238/238 [==============================] - 20s 83ms/step - loss: 1.2506 - acc: 0.4647 - val_loss: 1.3157 - val_acc: 0.4389\n",
      "Epoch 20/20\n",
      "238/238 [==============================] - 19s 78ms/step - loss: 1.2607 - acc: 0.4660 - val_loss: 1.2396 - val_acc: 0.4618\n"
     ]
    }
   ],
   "source": [
    "network_G.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'] )\n",
    "\n",
    "hist_g = network_G.fit(x_train,y_train, epochs=no_epochs, batch_size=batch_size, validation_data= (x_val,y_val))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T10:04:11.997319Z",
     "end_time": "2023-05-05T10:10:33.024128Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1491/1491 [==============================] - 18s 12ms/step - loss: 1.2008 - acc: 0.4876\n",
      "Accuracy on test set: 0.488\n"
     ]
    }
   ],
   "source": [
    "res = network_G.evaluate(x_test, y_test, steps=testExamples, verbose=1)\n",
    "print('Accuracy on test set: %.3f' % res[1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T10:10:33.028130Z",
     "end_time": "2023-05-05T10:10:50.925824Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T10:10:50.926825Z",
     "end_time": "2023-05-05T10:10:50.940825Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
